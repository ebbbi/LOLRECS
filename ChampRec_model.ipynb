{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random, os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "import easydict\n",
    "import math\n",
    "from itertools import chain\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 1000\n",
    "h_dim = 64\n",
    "num_layers = 1\n",
    "num_heads = 4\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 256\n",
    "dropout = 0.2\n",
    "save_dir_path = \"./model_saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len, device):\n",
    "        super(PositionalEncoding, self).__init__() \n",
    "        self.pe = torch.zeros(max_len, dim).to(device)\n",
    "        self.pe.requires_grad = False\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1).to(device)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim)).to(device)\n",
    "        self.pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x+self.pe[:x.size()[1], :]\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, dim=h_dim, dropout=dropout):\n",
    "        super(FFN, self).__init__()\n",
    "        self.linear1 = nn.Linear(dim, dim) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(dim, dim) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class ChampRec(nn.Module):\n",
    "    def __init__(self, n_ch=164, seq_len=9, dim=h_dim, num_heads=num_heads, num_layers=num_layers, dropout=dropout):\n",
    "        super(ChampRec, self).__init__()\n",
    "        self.pos_emb = nn.Embedding(seq_len, dim)\n",
    "        self.champ_emb = nn.Embedding(n_ch, dim)\n",
    "        self.pe = PositionalEncoding(dim, seq_len, device)\n",
    "        self.emb_linear1 = nn.Linear(dim, dim)\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(dim, num_heads, dim, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.layer_norm = nn.LayerNorm(dim)\n",
    "        self.FFN = FFN(dim, dropout)\n",
    "        self.final_layer = nn.Linear(dim, n_ch)\n",
    "    \n",
    "    def forward(self, champ_seq, pos):\n",
    "        champid_emb = self.champ_emb(champ_seq)\n",
    "        champid_emb = self.emb_linear1(champid_emb)\n",
    "        \n",
    "        champid_emb = self.pe(champid_emb)\n",
    "        pad_mask = (champ_seq == 0)\n",
    "\n",
    "        champid_emb = champid_emb.permute(1, 0, 2)\n",
    "        output = self.transformer_encoder(champid_emb, src_key_padding_mask=pad_mask)\n",
    "        output = output.permute(1, 0, 2)\n",
    "\n",
    "        output = output[:, -1]    \n",
    "        output = self.layer_norm(output) \n",
    "               \n",
    "        ffn_out = self.FFN(output)\n",
    "        \n",
    "        output = self.layer_norm(ffn_out + output)\n",
    "        output = self.final_layer(output).squeeze(1)\n",
    "        \n",
    "        output = torch.sigmoid(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, seq_sample, target, seq_len=9):  \n",
    "        self.seq_sample = seq_sample\n",
    "        self.target = target\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_sample)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.seq_sample[index]\n",
    "        t = self.target[index]\n",
    "                 \n",
    "        return torch.tensor(x), torch.tensor(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(ally, enemy, train = True):\n",
    "    seq_sample = []\n",
    "    target = []\n",
    "    test_pos = []\n",
    "    if train: \n",
    "        for x, o in zip(ally.ch, enemy.ch):\n",
    "            for i in range(4):\n",
    "                ax = [0 for _ in range(4)]\n",
    "                ex = [0 for _ in range(5)]\n",
    "                if i == 3:\n",
    "                    ex[-(i+2):] = o[:i+2]\n",
    "                else:\n",
    "                    ex[-(i+1):] = o[:i+1]\n",
    "                ax[-(i+1):] = x[:i+1]\n",
    "                target.append(x[i+1])\n",
    "                seq_sample.append(ex+ax)\n",
    "    else:\n",
    "        for x, o in zip(ally.ch, enemy.ch):\n",
    "            i = np.random.choice(range(4),1)[0]\n",
    "            ax = [0 for _ in range(4)]\n",
    "            ex = [0 for _ in range(5)]\n",
    "            if i == 3:\n",
    "                ex[-(i+2):] = o[:i+2]\n",
    "            else:\n",
    "                ex[-(i+1):] = o[:i+1]\n",
    "            ax[-(i+1):] = x[:i+1]\n",
    "            target.append(x[i+1])\n",
    "            seq_sample.append(ex+ax)\n",
    "            test_pos.append(p[i+1])\n",
    "            \n",
    "    return seq_sample, target, test_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(answer, pred):\n",
    "    recall = 0.0\n",
    "    for a, p in zip(answer, pred):\n",
    "        ans_set = set([a.item()])\n",
    "        pred_set = set(p)\n",
    "        recall += len(ans_set & pred_set) / float(len(ans_set))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_test(answer, toplist):\n",
    "    return len(set([answer]) & set(toplist)) / len([answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def keystoint(x):\n",
    "    return {int(k): v for k, v in x}\n",
    "\n",
    "with open('../encdec_pos.json', 'r', encoding='utf-8') as f:\n",
    "    json_string = f.read()\n",
    "    \n",
    "ch_pos = json.loads(json_string, object_pairs_hook=keystoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"transformer_train.parquet\")\n",
    "op_train= pd.read_parquet(\"transformer_op_train.parquet\")\n",
    "valid = pd.read_parquet(\"transformer_valid.parquet\")\n",
    "op_valid = pd.read_parquet(\"transformer_op_valid.parquet\")\n",
    "test = pd.read_parquet(\"transformer_test.parquet\")\n",
    "op_test = pd.read_parquet(\"transformer_op_test.parquet\")\n",
    "pos_test = pd.read_parquet(\"transformer_pos_test.parquet\")\n",
    "ban_test = pd.read_parquet(\"transformer_ban_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for x, o in zip(train.ch, op_train.ch):\n",
    "    for i in range(4):\n",
    "        ax = [0 for _ in range(4)]\n",
    "        ex = [0 for _ in range(5)]\n",
    "        if i == 3:\n",
    "            ex[-(i+2):] = o[:i+2]\n",
    "        else:\n",
    "            ex[-(i+1):] = o[:i+1]\n",
    "        ax[-(i+1):] = x[:i+1]\n",
    "        y_train.append(x[i+1])\n",
    "        X_train.append(ex+ax)\n",
    "        \n",
    "X_valid = []\n",
    "y_valid = []\n",
    "for x, o in zip(valid.ch, op_valid.ch):\n",
    "    i = np.random.choice(range(4), 1)[0]\n",
    "    ax = [0 for _ in range(4)]\n",
    "    ex = [0 for _ in range(5)]\n",
    "    if i == 3:\n",
    "        ex[-(i+2):] = o[:i+2]\n",
    "    else:\n",
    "        ex[-(i+1):] = o[:i+1]\n",
    "    ax[-(i+1):] = x[:i+1]\n",
    "    y_valid.append(x[i+1])\n",
    "    X_valid.append(ex+ax)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "test_pos = []\n",
    "for x, o, p in zip(test.ch, op_test.ch, pos_test.pos):\n",
    "    i = np.random.choice(range(4), 1)[0]\n",
    "    ax = [0 for _ in range(4)]\n",
    "    ex = [0 for _ in range(5)]\n",
    "    if i == 3:\n",
    "        ex[-(i+2):] = o[:i+2]\n",
    "    else:\n",
    "        ex[-(i+1):] = o[:i+1]\n",
    "    ax[-(i+1):] = x[:i+1]\n",
    "    y_test.append(x[i+1])\n",
    "    X_test.append(ex+ax)\n",
    "    test_pos.append(p[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(SeqDataset(X_train, y_train), batch_size=batch_size)\n",
    "valid_loader = DataLoader(SeqDataset(X_valid, y_valid), batch_size=batch_size)\n",
    "test_loader = DataLoader(SeqDataset(X_test, y_test), batch_size=len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCELoss()\n",
    "model = ChampRec().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "best_recall = 0\n",
    "cnt = 0\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for x, target in train_loader:\n",
    "        x = x.to(device).long()\n",
    "        target = target.to(device).long()\n",
    "        pos = torch.arange(9).unsqueeze(0).to(device)\n",
    "\n",
    "        label = torch.zeros((target.shape[0], 164)).to(device)        \n",
    "        label.scatter_(1, target[:, None], 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(x, pos)\n",
    "        loss = loss_func(preds, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "    train_loss = np.mean(train_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    recall = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, target in valid_loader:\n",
    "            x = x.to(device).long()\n",
    "            pos = torch.arange(9).unsqueeze(0).to(device)\n",
    "            target = target.to(device).long()\n",
    "            \n",
    "            preds = model(x, pos)\n",
    "            _, items = torch.topk(preds, 3)\n",
    "            items = items.to(\"cpu\").detach().numpy()\n",
    "            recall += recall_at_k(target, items)\n",
    "            \n",
    "        recall /= len(valid_loader.dataset)    \n",
    "        \n",
    "        if recall > best_recall:\n",
    "            best_recall = recall\n",
    "            cnt = 0\n",
    "            if not os.path.exists(save_dir_path):\n",
    "                os.makedirs(save_dir_path)\n",
    "            model_scripted = torch.jit.script(model) #Export to TorchScript\n",
    "            model_scripted.save('model_scripted_fixed_length.pt') # Save\n",
    "        else:\n",
    "            cnt+=1\n",
    "            \n",
    "    train_losses.append(train_loss)\n",
    "    print(f\"epoch : {epoch} Train Loss : {train_loss} recall : {recall}\")\n",
    "    if cnt >= 50:\n",
    "        print('Early Stopping!')\n",
    "        print(best_recall)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "top3recall = 0.0\n",
    "test_loader = DataLoader(SeqDataset(X_test, y_test), batch_size=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (x, target), ban in zip(test_loader, ban_test.ch.values):\n",
    "        x = x.to(device).long()\n",
    "        target = target.to(device).long()\n",
    "        ban = torch.from_numpy(ban).to(device).unsqueeze(0).long()\n",
    "        pos = torch.arange(9).unsqueeze(0).to(device)\n",
    "        \n",
    "        preds = model(x, pos)\n",
    "        \n",
    "        mask = torch.zeros(1, 164).to(device).long()\n",
    "        mask.scatter_(1, x, 1)\n",
    "        mask.scatter_(1, ban, 1)\n",
    "        preds[mask==1] = -np.inf\n",
    "\n",
    "        items = torch.argsort(preds, descending=True, dim = 1)\n",
    "        items = items.to(\"cpu\").detach().numpy()[0]\n",
    "        \n",
    "        top3recall+=recall_test(target.item(), items[:3])            \n",
    "     \n",
    "    print(top3recall / len(test_loader.dataset))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "top3recall = 0.0\n",
    "posrecall = 0.0\n",
    "top15recall = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (x, target), ban, rp in zip(test_loader, ban_test.ch.values, test_pos):\n",
    "        x = x.to(device).long()\n",
    "        target = target.to(device).long()\n",
    "        ban = torch.from_numpy(ban).to(device).unsqueeze(0).long()\n",
    "        pos = torch.arange(9).unsqueeze(0).to(device)\n",
    "        \n",
    "        preds = model(x, pos)\n",
    "        \n",
    "        mask = torch.zeros(1, 164).to(device).long()\n",
    "        mask.scatter_(1, x, 1)\n",
    "        mask.scatter_(1, ban, 1)\n",
    "        preds[mask == 1] = -np.inf\n",
    "\n",
    "        items = torch.argsort(preds, descending=True, dim = 1)\n",
    "        items = items.to(\"cpu\").detach().numpy()[0]\n",
    "\n",
    "        posrec = [[], [], [], [], []]\n",
    "        \n",
    "        total_len = 0\n",
    "        for x in items:\n",
    "            if total_len == 15:\n",
    "                break\n",
    "            for pos in ch_pos[x]:\n",
    "                if len(posrec[pos]) < 3:\n",
    "                    posrec[pos].append(x)\n",
    "                    total_len+=1\n",
    "                    \n",
    "        top3recall += recall_test(target.item(), items[:3])      \n",
    "        posrecall += recall_test(target.item(), posrec[rp])\n",
    "        top15recall += recall_test(target.item(), chain.from_iterable(posrec))        \n",
    "     \n",
    "    print(top3recall / len(test_loader.dataset))   \n",
    "    print(posrecall / len(test_loader.dataset))   \n",
    "    print(top15recall / len(test_loader.dataset))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
